# 🤖 RAG Chatbot in Amazon Bedrock

## 🚀 Project Overview
This project demonstrates how to set up a **Retrieval-Augmented Generation (RAG) chatbot** in **Amazon Bedrock**, leveraging **Amazon S3, OpenSearch Serverless**, and **Titan text embeddings v2** for efficient knowledge retrieval.

## 🛠️ Technologies Used
- **Amazon Bedrock** – AI model marketplace for generative AI applications  
- **Amazon S3** – Storage for chatbot knowledge base  
- **OpenSearch Serverless** – Vector search for retrieving relevant chunks of data  
- **Llama 3.3 70B** – AI model for natural language processing  
- **Titan Text Embeddings v2** – Fast and accurate embedding model for vector search  

## 📌 Key Features
✔️ **Retrieval-Augmented Generation (RAG):** Combines knowledge retrieval with generative AI  
✔️ **Semantic Search with OpenSearch:** Finds relevant chunks of text based on meaning  
✔️ **Scalable Knowledge Base:** Syncs with **Amazon S3** and updates dynamically  
✔️ **Custom AI Model Access:** Uses **Llama 3.3 70B** for high-quality chatbot responses  

## 🏗️ How It Works
1. **Upload Knowledge Base:** Store documents in **Amazon S3**  
2. **Embed & Chunk Data:** Convert text into vector representations  
3. **Store in OpenSearch:** Enables fast and meaningful retrieval  
4. **AI-Powered Responses:** The chatbot generates intelligent answers  

## 🛠️ Challenges & Learnings
- **Model Access Error:** Llama 3.1 8B wasn’t available, so I switched to Llama 3.3 70B  
- **Syncing Data:** Ensuring proper chunking and embeddings improved search accuracy  
- **Understanding Amazon Bedrock:** Explored how AWS handles generative AI models  

---
🔗 **Connect with me on LinkedIn**: https://www.linkedin.com/in/pavan-gaggera
